{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, dim, dim_k, dim_out, heads):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "\n",
    "        assert(dim_out % heads) == 0\n",
    "        heads = heads\n",
    "        dim_v = dim_out // heads\n",
    "\n",
    "# (Page 5) BN after Q and V are helpful\n",
    "        get_q = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=dim, out_channels=dim_k*heads, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(dim_k*heads),\n",
    "        )\n",
    "        get_v = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=dim, out_channels=dim_v, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(dim_v),\n",
    "        )\n",
    "        self.get_k = nn.Conv2d(in_channels=dim, out_channels=dim_k*heads, kernel_size=1, bias=False)\n",
    "\n",
    "        ## TODO： \n",
    "        # embedding = ()\n",
    "        # self.relative_position = generate_relative_positions_matrix(dim,dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        (b, c, im_h, im_w) = x.shape\n",
    "        Q = self.get_q(x)\n",
    "        K = self.get_k(x)\n",
    "        V = self.get_v(x)\n",
    "        \n",
    "        Q = rearrange(Q, 'b (h k) im_h im_w -> b h k (im_h im_w)', h=self.heads)\n",
    "        print(Q.size())\n",
    "        K = rearrange(K, 'b k im_h im_w -> b k (im_h im_w)')\n",
    "        print(K.size())\n",
    "        V = rearrange(V, 'b v im_h im_w -> b v (im_h im_w)')\n",
    "        print(V.size())\n",
    "        σ_K = K.softmax(dim=-1)\n",
    "        λc = einsum('b k m, b v m -> b k v', σ_K, V)\n",
    "        # λp = einsum('b ') ## TODO: 添加Embedding\n",
    "\n",
    "        # λn = λc + λp\n",
    "        return λc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 65025])\n",
      "torch.Size([32, 9, 65025])\n",
      "torch.Size([32, 1, 65025])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torch import einsum\n",
    "from torchinfo import summary\n",
    "\n",
    "# summary(LambdaLayer(dim=3,dim_k=3,dim_out=3,heads=3), (32,3,255,255))\n",
    "\n",
    "X = torch.rand((32,3,255,255))\n",
    "LambdaLayer(dim=3,dim_k=3,dim_out=3,heads=3)(X).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "def rel_pos_indices(size):\n",
    "    size = (size, size)\n",
    "    pos = torch.stack(torch.meshgrid(torch.arange(size[0]), torch.arange(size[1]))).flatten(1)\n",
    "    rel_pos = pos[:, None, :] - pos[:, :, None]\n",
    "    rel_pos[0] += size[0] - 1\n",
    "    rel_pos[1] += size[1] - 1\n",
    "    return rel_pos  # 2, H * W, H * W\n",
    "\n",
    "def get_relative_position_matrix(size,):\n",
    "    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "    y = torch.arange(size).repeat(size)\n",
    "    distance_mat = torch.stack((x,y))\n",
    "    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "    distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "    distance_mat += size-1\n",
    "    return distance_mat\n",
    "\n",
    "rel_pos = get_relative_position_matrix(64)\n",
    "rel_pos_2 = rel_pos_indices(64)\n",
    "rel_lengths = 2 * 64 - 1 # n = im_h = im_w the feature map size\n",
    "rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, 16)) # 2*n-1 2*n-1 k\n",
    "\n",
    "torch.equal(rel_pos_emb[rel_pos[0],rel_pos[1]], rel_pos_emb[rel_pos_2[0],rel_pos_2[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 2, 2],\n",
      "         [1, 1, 2, 2],\n",
      "         [0, 0, 1, 1],\n",
      "         [0, 0, 1, 1]],\n",
      "\n",
      "        [[1, 2, 1, 2],\n",
      "         [0, 1, 0, 1],\n",
      "         [1, 2, 1, 2],\n",
      "         [0, 1, 0, 1]]])\n",
      "tensor([[[1, 1, 2, 2],\n",
      "         [1, 1, 2, 2],\n",
      "         [0, 0, 1, 1],\n",
      "         [0, 0, 1, 1]],\n",
      "\n",
      "        [[1, 2, 1, 2],\n",
      "         [0, 1, 0, 1],\n",
      "         [1, 2, 1, 2],\n",
      "         [0, 1, 0, 1]]])\n",
      "tensor([[[1, 1],\n",
      "         [1, 2],\n",
      "         [2, 1],\n",
      "         [2, 2]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 1],\n",
      "         [2, 0],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [0, 2],\n",
      "         [1, 1],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[0, 0],\n",
      "         [0, 1],\n",
      "         [1, 0],\n",
      "         [1, 1]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "def rel_pos_indices(size):\n",
    "    size = (size, size)\n",
    "    pos = torch.stack(torch.meshgrid(torch.arange(size[0]), torch.arange(size[1]))).flatten(1)\n",
    "    rel_pos = pos[:, None, :] - pos[:, :, None]\n",
    "    rel_pos[0] += size[0] - 1\n",
    "    rel_pos[1] += size[1] - 1\n",
    "    return rel_pos  # 2, H * W, H * W\n",
    "\n",
    "  \n",
    "from einops import rearrange\n",
    "def get_relative_position_matrix(size,):\n",
    "  x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "  y = torch.arange(size).repeat(size)\n",
    "  distance_mat = torch.stack((x,y))\n",
    "  distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "  distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "  distance_mat += size-1\n",
    "  return distance_mat\n",
    "\n",
    "def calc_rel_pos(n):\n",
    "    pos = torch.meshgrid(torch.arange(n), torch.arange(n))\n",
    "    pos = rearrange(torch.stack(pos), 'n i j -> (i j) n')  # [n*n, 2] pos[n] = (i, j)\n",
    "    rel_pos = pos[None, :] - pos[:, None]                  # [n*n, n*n, 2] rel_pos[n, m] = (rel_i, rel_j)\n",
    "    rel_pos += n - 1                                       # shift value range from [-n+1, n-1] to [0, 2n-2]\n",
    "    return rel_pos\n",
    "\n",
    "print(get_relative_position_matrix(2))\n",
    "print(rel_pos_indices(2))\n",
    "print(calc_rel_pos(2))\n",
    "torch.equal(get_relative_position_matrix(10), rel_pos_indices(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 2, 2],\n",
       "         [1, 1, 2, 2],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 1, 1]],\n",
       "\n",
       "        [[1, 2, 1, 2],\n",
       "         [0, 1, 0, 1],\n",
       "         [1, 2, 1, 2],\n",
       "         [0, 1, 0, 1]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=2\n",
    "\n",
    "x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "y = torch.arange(size).repeat(size)\n",
    "distance_mat = torch.stack((x,y))\n",
    "distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "# distance_mat = rearrange(distance_mat, 'n i j -> (i j) n')\n",
    "distance_mat += size-1\n",
    "distance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torch import einsum\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "    #  np.random.seed(seed)\n",
    "    #  random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)\n",
    "\n",
    "def calc_rel_pos(n):\n",
    "    pos = torch.meshgrid(torch.arange(n), torch.arange(n))\n",
    "    pos = rearrange(torch.stack(pos), 'n i j -> (i j) n')  # [n*n, 2] pos[n] = (i, j)\n",
    "    rel_pos = pos[None, :] - pos[:, None]                  # [n*n, n*n, 2] rel_pos[n, m] = (rel_i, rel_j)\n",
    "    rel_pos += n - 1                                       # shift value range from [-n+1, n-1] to [0, 2n-2]\n",
    "    return rel_pos\n",
    "\n",
    "def calc_embeddings(dim_k, n):\n",
    "    rel_lengths = 2 * n - 1\n",
    "    # rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k))\n",
    "    rel_pos_emb = nn.Parameter(torch.arange(rel_lengths*rel_lengths*dim_k,dtype=float).reshape(rel_lengths,rel_lengths,dim_k))\n",
    "    rel_pos = calc_rel_pos(n)\n",
    "\n",
    "    n, m = rel_pos.unbind(dim = -1)\n",
    "    rel_pos_emb = rel_pos_emb[n, m]\n",
    "    return rel_pos_emb\n",
    "\n",
    "def get_relative_position_matrix(size,):\n",
    "  x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "  y = torch.arange(size).repeat(size)\n",
    "  distance_mat = torch.stack((x,y))\n",
    "  distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "  distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "  distance_mat += size-1\n",
    "  return distance_mat\n",
    "\n",
    "def get_embedding(dim_k=1, n=2):\n",
    "  rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n",
    "  # rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k)) # 2*n-1 2*n-1 k \n",
    "  rel_pos_emb = nn.Parameter(torch.arange(rel_lengths*rel_lengths*dim_k,dtype=float).reshape(rel_lengths,rel_lengths,dim_k))\n",
    "  rel_pos = get_relative_position_matrix(n)\n",
    "  return rel_pos_emb[rel_pos[0], rel_pos[1]]\n",
    "\n",
    "# print(calc_embeddings(1,10))\n",
    "# print(get_embedding(1,10))\n",
    "# torch.equal(calc_embeddings(1,10), get_embedding(1,10))\n",
    "print(get_embedding(1,10).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from modeling.ResNet import LambdaLayer\n",
    "from lambda_networks import LambdaLayer\n",
    "\n",
    "layer1 = LambdaLayer(\n",
    "    dim = 32,       # channels going in\n",
    "    dim_out = 32,   # channels out\n",
    "    n = 64,         # size of the receptive window - max(height, width)\n",
    "    dim_k = 16,     # key dimension\n",
    "    heads = 4,      # number of heads, for multi-query\n",
    "    dim_u = 1       # 'intra-depth' dimension\n",
    ")\n",
    "\n",
    "#  dim, dim_k, n, dim_out, heads, r=None\n",
    "layer2 = LambdaLayer(\n",
    "    dim = 32,       # channels going in\n",
    "    dim_k = 16,     # key dimension\n",
    "    n = 64,         # size of the receptive window - max(height, width)\n",
    "    dim_out = 32,   # channels out\n",
    "    heads = 4,      # number of heads, for multi-query\n",
    "    # r = 23,\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 32, 64, 64)\n",
    "print(layer1(x).shape)\n",
    "print(layer2(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/f44w08g92cddctb47s9_02jw0000gn/T/ipykernel_55840/2759426288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "def get_relative_position_matrix(size,):\n",
    "    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "    y = torch.arange(size).repeat(size)\n",
    "    distance_mat = torch.stack((x,y))\n",
    "    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "    distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "    distance_mat += size-1\n",
    "    return distance_mat\n",
    "\n",
    "def get_embedding(dim_k, n):\n",
    "    rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n",
    "    rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k)) # 2*n-1 2*n-1 k \n",
    "    rel_pos = get_relative_position_matrix(n)\n",
    "    # return rel_pos\n",
    "    return rel_pos_emb[rel_pos[0], rel_pos[1]]\n",
    "\n",
    "\n",
    "n, m = get_embedding(16,64).unbind(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "from torch import einsum\n",
    "import torch.nn as nn\n",
    "def get_relative_position_matrix(size,):\n",
    "    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "    y = torch.arange(size).repeat(size)\n",
    "    distance_mat = torch.stack((x,y))\n",
    "    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "    distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "    distance_mat += size-1\n",
    "    return distance_mat\n",
    "\n",
    "def get_embedding(dim_k, n):\n",
    "    rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n",
    "    rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k)) # 2*n-1 2*n-1 k \n",
    "    rel_pos = get_relative_position_matrix(n)\n",
    "    # return rel_pos\n",
    "    return rel_pos_emb[rel_pos[0], rel_pos[1]]\n",
    "\n",
    "V = torch.randn(1, 64*64, 32//4)\n",
    "embeddings = get_embedding(16, 64) # n m k\n",
    "λp = einsum('n m k, b m v -> b n k v', embeddings, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 64, 224, 224] to have 3 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/f44w08g92cddctb47s9_02jw0000gn/T/ipykernel_88676/2890983089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmaxpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 64, 224, 224] to have 3 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "x = torch.rand(1,3,224,224)\n",
    "\n",
    "conv1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
    "                        bias=False),\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
    "                    bias=False),                  \n",
    ")\n",
    "\n",
    "bn1 = nn.BatchNorm2d(64)\n",
    "relu = nn.ReLU(inplace=True)\n",
    "maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "x = conv1(x)\n",
    "x = bn1(x)\n",
    "x = relu(x)\n",
    "x = maxpool(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3136, 3136, 16])\n",
      "torch.Size([1, 3136, 16])\n",
      "torch.Size([3136, 3136, 16])\n",
      "torch.Size([1, 3136, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9174, -0.0215, -0.8486, -0.0110, -0.0028, -0.7266,  0.8609, -0.4256,\n",
       "          0.7112,  0.7506]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling.ResNet import resnet18\n",
    "import torch\n",
    "model = resnet18()\n",
    "x = torch.rand(1,3,224,224)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3136, 3136, 16])\n",
      "torch.Size([1, 3136, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from torch import einsum\n",
    "\n",
    "def get_relative_position_matrix(size,):\n",
    "    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "    y = torch.arange(size).repeat(size)\n",
    "    distance_mat = torch.stack((x,y))\n",
    "    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "    distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "    distance_mat += size-1\n",
    "    return distance_mat\n",
    "\n",
    "k=16\n",
    "r=23\n",
    "b=1\n",
    "im_h=im_w=224\n",
    "n = 56\n",
    "dim_v=56//2\n",
    "\n",
    "rel_pos = get_relative_position_matrix(n)\n",
    "rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n",
    "rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, k)) # 2*n-1 2*n-1 k \n",
    "\n",
    "Q = torch.rand(1,4,224*224,16)\n",
    "V = torch.rand(1,56*56,dim_v)\n",
    "# pos_conv = nn.Conv3d(1, k, (r, r, 1), padding=(r // 2, r // 2, 0))\n",
    "# V = rearrange(V, 'b (im_h im_w) v -> b v im_h im_w', im_h = im_h, im_w = im_w).reshape(b, 1, dim_v, im_h, im_w,)\n",
    "# embeddings = pos_conv(V)\n",
    "# # embeddings.flatten(3).shape\n",
    "# # Q.shape\n",
    "# position_output = einsum('b h n k, b k v n -> b n h v', Q, embeddings.flatten(3))\n",
    "# position_output.shape\n",
    "\n",
    "# 56x56=3136\n",
    "embeddings = rel_pos_emb[rel_pos[0], rel_pos[1]]\n",
    "print(embeddings.shape)\n",
    "print(V.shape)\n",
    "λp = einsum('n m k, b m v -> b n k v', embeddings, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n",
      "[-4  3]\n",
      "[ 3 -4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "y = np.array([5,7,10,6,9])\n",
    "y_m = np.array([2,3,-4,3])\n",
    "def AR(y):\n",
    "    y_process=np.lib.stride_tricks.sliding_window_view(y,(2))\n",
    "    output = []\n",
    "    for y in y_process:\n",
    "        y = y[::-1]\n",
    "        output.append(0.8*y[0] + 0.3*y[1])\n",
    "\n",
    "\n",
    "AR(y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "csv_file = '/Volumes/Loading/datasets/imagewoof2/noisy_imagewoof.csv'\n",
    "image_path_label = pd.read_csv(csv_file)\n",
    "train_path_label = image_path_label.loc[image_path_label['is_valid'] == 0]\n",
    "val_path_label = image_path_label.loc[image_path_label['is_valid'] == 1]\n",
    "\n",
    "train_path_label.to_csv('train_noisy_imagewoof.csv')\n",
    "val_path_label.to_csv('val_noisy_imagewoof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n",
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "from torchvision import transforms\n",
    "\n",
    "image_root = '/Volumes/Loading/imagenette2'\n",
    "csv_file = '/Volumes/Loading/imagenette2/noisy_imagenette.csv'\n",
    "\n",
    "image_path_label = pd.read_csv(csv_file)\n",
    "train_path_label = image_path_label.loc[image_path_label['is_valid'] == 0]\n",
    "val_path_label = image_path_label.loc[image_path_label['is_valid'] == 1]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomErasing(0.2),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class ImageNette(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, noisy_level=0, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.image_path_list = csv_file\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.noisy_level = noisy_level\n",
    "        self.train_map = {\n",
    "            'n02979186': 0,\n",
    "            'n03417042': 1,\n",
    "            'n01440764': 2,\n",
    "            'n02102040': 3,\n",
    "            'n03028079': 4,\n",
    "            'n03888257': 5,\n",
    "            'n03394916': 6,\n",
    "            'n03000684': 7,\n",
    "            'n03445777': 8,\n",
    "            'n03425413': 9\n",
    "        }\n",
    "        self.val_map = {\n",
    "            'n02979186': 0,\n",
    "            'n03417042': 1,\n",
    "            'n01440764': 2,\n",
    "            'n02102040': 3,\n",
    "            'n03028079': 4,\n",
    "            'n03888257': 5,\n",
    "            'n03394916': 6,\n",
    "            'n03000684': 7,\n",
    "            'n03445777': 8,\n",
    "            'n03425413': 9\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.root_dir,\n",
    "                                self.image_path_list['path'][idx])\n",
    "        image = Image.open(img_path)\n",
    "        labels_head = 'noisy_labels_'+str(self.noisy_level)\n",
    "        label = self.val_map[self.image_path_list[labels_head][idx]]\n",
    "        label = torch.from_numpy(np.array([label]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample =  [image, label]\n",
    "        return sample\n",
    "\n",
    "transformed_dataset = ImageNette(\n",
    "    csv_file=train_path_label,\n",
    "    root_dir=image_root,\n",
    "    noisy_level=0,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    transformed_dataset, batch_size=6\n",
    ")\n",
    "from tqdm import tqdm\n",
    "tk = tqdm(train_dataloader)\n",
    "for t, data in enumerate(tk):\n",
    "    print(data[0].shape)\n",
    "    print(data[1].flatten())\n",
    "    break\n",
    "# for i in range(len(transformed_dataset)):\n",
    "#     sample = transformed_dataset[i]\n",
    "\n",
    "#     print(i, sample[0].size(), sample[1].size())\n",
    "\n",
    "#     if i == 3:\n",
    "#         break\n",
    "# index = F.one_hot(torch.arange(0, 10), num_classes=10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3588)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3588)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def softXEnt (input, target):\n",
    "    logprobs = torch.nn.functional.log_softmax (input, dim = 1)\n",
    "    return  -(target * logprobs).sum() / input.shape[0]\n",
    "\n",
    "x = torch.randn(10,3)\n",
    "y = torch.eye(10,3)\n",
    "\n",
    "print(torch.nn.functional.cross_entropy(x, y))\n",
    "softXEnt(x,y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n02115641': 0,\n",
       " 'n02086240': 1,\n",
       " 'n02088364': 2,\n",
       " 'n02087394': 3,\n",
       " 'n02105641': 4,\n",
       " 'n02111889': 5,\n",
       " 'n02099601': 6,\n",
       " 'n02096294': 7,\n",
       " 'n02093754': 8,\n",
       " 'n02089973': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "# csv_file = '/Volumes/Loading/imagenette2/noisy_imagenette.csv'\n",
    "csv_file = '/Volumes/Loading/datasets/imagewoof2/noisy_imagewoof.csv'\n",
    "image_root = '/Volumes/Loading/imagewoof2'\n",
    "df = pd.read_csv(csv_file)\n",
    "# idx = 1\n",
    "# im_path = os.path.join(image_root, df['path'][idx])\n",
    "# Image.open(im_path)\n",
    "labels = list(df.loc[df['is_valid'] == 1]['noisy_labels_0'].unique())\n",
    "index = np.arange(0, len(labels))\n",
    "dict(zip(labels, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "# from lambda_networks import LambdaLayer\n",
    "from torch import einsum\n",
    "# make required imports\n",
    "import torch \n",
    "import timm \n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataclasses import dataclass\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.helpers import build_model_with_cfg\n",
    "from timm.models.resnet import Bottleneck, _create_resnet, default_cfgs, _cfg, make_blocks, create_classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "# from lambda_networks import LambdaLayer\n",
    "from torch import einsum\n",
    "\n",
    "# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "def get_relative_position_matrix(size,):\n",
    "    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n",
    "    y = torch.arange(size).repeat(size)\n",
    "    distance_mat = torch.stack((x,y))\n",
    "    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n",
    "    distance_mat = torch.clamp(distance_mat, -size, size)\n",
    "    distance_mat += size-1\n",
    "    return distance_mat\n",
    "\n",
    "class LambdaLayer_own(nn.Module):\n",
    "    def __init__(self, dim, dim_k=16,n=64, dim_out=None, heads=4, r=None):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.r = r\n",
    "        self.n = n\n",
    "        self.k = dim_k\n",
    "        assert(dim_out % heads) == 0\n",
    "        self.heads = heads\n",
    "        self.dim_v = dim_out // heads\n",
    "\n",
    "# (Page 5) BN after Q and V are helpful\n",
    "        self.get_q = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=dim, out_channels=dim_k*heads, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(dim_k*heads),\n",
    "        )\n",
    "        self.get_v = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=dim, out_channels=self.dim_v, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_v),\n",
    "        )\n",
    "        self.get_k = nn.Conv2d(in_channels=dim, out_channels=dim_k, kernel_size=1, bias=False)\n",
    "\n",
    "        if r is not None:\n",
    "            assert (r % 2) == 1, 'Receptive kernel size should be odd'\n",
    "            self.pos_conv = nn.Conv3d(1, self.k, (r, r, 1), padding=(r // 2, r // 2, 0))\n",
    "        else:   \n",
    "            # rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n",
    "            # self.rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k)) # n m k \n",
    "            self.rel_pos = get_relative_position_matrix(n)\n",
    "            rel_lengths = 2 * self.n - 1 # n = im_h = im_w the feature map size\n",
    "            self.rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, self.k)) # 2*n-1 2*n-1 k \n",
    "\n",
    "    def forward(self,x):\n",
    "        (b, c, im_h, im_w) = x.shape\n",
    "        Q = self.get_q(x)\n",
    "        K = self.get_k(x)\n",
    "        V = self.get_v(x)\n",
    "\n",
    "        # m = n = im_h * im_w\n",
    "        Q = rearrange(Q, 'b (h k) im_h im_w -> b h (im_h im_w) k', h=self.heads) # b h n(m) k \n",
    "        K = rearrange(K, 'b k im_h im_w -> b (im_h im_w) k') # b m k \n",
    "        V = rearrange(V, 'b v im_h im_w -> b (im_h im_w) v') # b m v\n",
    "\n",
    "        σ_K = K.softmax(dim=-1)\n",
    "        λc = einsum('b m k, b m v -> b k v', σ_K, V)\n",
    "        content_output = einsum('b h n k, b k v -> b n h v', Q, λc)\n",
    "\n",
    "        if self.r is not None:\n",
    "            V = rearrange(V, 'b (im_h im_w) v -> b v im_h im_w', im_h = im_h, im_w = im_w).reshape(b, 1, self.dim_v, im_h, im_w,)\n",
    "            embeddings = self.pos_conv(V)\n",
    "            position_output = einsum('b h n k, b k v n -> b n h v', Q, embeddings.flatten(3))\n",
    "        else:\n",
    "            # embeddings = get_embedding(self.k, im_h) # n m k\n",
    "            embeddings = self.rel_pos_emb[self.rel_pos[0], self.rel_pos[1]]\n",
    "#             print(embeddings.shape)\n",
    "#             print(V.shape)\n",
    "            λp = einsum('n m k, b m v -> b n k v', embeddings, V)\n",
    "            position_output = einsum('b h n k, b n k v -> b n h v',Q, λp)\n",
    "\n",
    "        output = rearrange(content_output + position_output, 'b (im_h im_w) h v -> b (h v) im_h im_w', im_h=im_h, im_w=im_w)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,feature_size=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.conv2 = conv3x3(planes, planes)\n",
    "#         self.conv2 = LambdaLayer(\n",
    "#                             dim = planes,\n",
    "#                             dim_out = planes,\n",
    "#                             n = feature_size,\n",
    "# #                             r = 23,         # the receptive field for relative positional encoding (23 x 23)\n",
    "#                             dim_k = 16,\n",
    "#                             heads = 4,\n",
    "#                             dim_u = 4\n",
    "#                         )\n",
    "        self.conv2 = LambdaLayer_own(dim=planes, dim_out=planes, n=feature_size)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BasicBlock_Rs(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,feature_size=None):\n",
    "        super(BasicBlock_Rs, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        # self.conv2 = LambdaLayer(dim=planes, dim_out=planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block_lambda, block_rs, layers, num_classes=10, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None,):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        # self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "        #                        bias=False)\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block_rs, 64, layers[0], feature_size=56)\n",
    "        self.layer2 = self._make_layer(block_rs, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0],feature_size=28) # 56/2=\n",
    "        self.layer3 = self._make_layer(block_lambda, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1],feature_size=14) # 14\n",
    "        self.layer4 = self._make_layer(block_rs, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2],feature_size=7) # 7\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block_rs.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False, feature_size=None):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer, feature_size=feature_size))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer,feature_size=feature_size))\n",
    "\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(arch, block,BasicBlock_Rs, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, BasicBlock_Rs, layers, **kwargs)\n",
    "    # if pretrained:\n",
    "    #     state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "    #                                           progress=progress)\n",
    "    #     model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock,BasicBlock_Rs, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "# [2, 2, 2, 2] RL-26\n",
    "# [2, 3, 5, 2] RL-38\n",
    "# [3, 4, 6, 3] RL-50\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock,BasicBlock_Rs, [2, 3, 5, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', BasicBlock, BasicBlock_Rs,[3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L R (r None)2\n",
    "18: 11012586  1.61GFlops\n",
    "34: 14765498  2.43GFlops\n",
    "\n",
    "# L R (r None) 3\n",
    "18: 10099562 1.61GFlops\n",
    "34: 12313882 2.43GFlops\n",
    "\n",
    "# L R (r 23)2\n",
    "18: 10932746  1.61GFlops\n",
    "34: 14645738  2.43GFlops\n",
    "\n",
    "# L R (r 23) 3\n",
    "18: 10093194 1.61GFlops\n",
    "34: 12297962 2.43GFlops\n",
    "\n",
    "# res\n",
    "18: 11689512\n",
    "34: 21797672"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "woof noisy25 resnet34 \n",
    "time: 3h 35m 37s\n",
    "train_loss: 2.238\n",
    "eval_loss: 2.295\n",
    "woof noisy25 resnet18\n",
    "time: 3h 7m 0s\n",
    "train_loss: 2.232\n",
    "eval_loss: 2.295\n",
    "\n",
    "woof noisy0 resnet34\n",
    "time: 2h 29m 54s\n",
    "train_loss: 2.303\n",
    "eval_loss: 2.282\n",
    "woof noisy0 resnet18\n",
    "time: 2h 6s\n",
    "train_loss: 2.301\n",
    "eval_loss: 2.287\n",
    "\n",
    "imagenette noisy5 resnet34\n",
    "time: 2h 21m 3s\n",
    "train_loss: 2.287\n",
    "eval_loss: 2.32\n",
    "\n",
    "imagenette noisy5 resnet18\n",
    "time: 1h 58m 53s\n",
    "train_loss: 2.307\n",
    "eval_loss: 2.317\n",
    "\n",
    "ImageNette\n",
    "R18_0 \n",
    "time: 1h 32m 37s\n",
    "train_loss: 2.1614\n",
    "eval_loss: 2.2887\n",
    "\n",
    "R34_0\n",
    "time: 1h 57m 2s\n",
    "train_loss: 2.3295\n",
    "eval_loss: 2.1858\n",
    "R18_50\n",
    "time: 1h 35m 42s\n",
    "train_loss: 2.3360\n",
    "eval_loss: 2.3483\n",
    "R34_50\n",
    "time: 1h 41m 4s\n",
    "train_loss: 2.3372\n",
    "eval_loss: 2.3442\n",
    "\n",
    "\n",
    "1. imagenette-noisy25-resnet18\n",
    "\truntime: 2h 40m 22s\n",
    "\tbest train_loss: 2.332\n",
    "\tbest eval_loss: 2.331\n",
    "\n",
    "2. imagenette-noisy25-resnet34\n",
    "\truntime: 1h 49m 59s\n",
    "\tbest train_loss: 2.324\n",
    "\tbest eval_loss: 2.33\n",
    "\n",
    "3. imagenette-noisy25-resnet50\n",
    "\truntime: 2h 23m 10s\n",
    "\tbest train_loss: 2.181\n",
    "\tbest eval_loss: 2.31\n",
    "\n",
    "4. imagewoof-noisy5-resnet18\n",
    "\truntime: 3h 11m 4s\n",
    "\tbest train_loss: 2.332\n",
    "\tbest eval_loss: 2.323\n",
    "\n",
    "5. imagewoof-noisy5-resnet34\n",
    "\truntime: 3h 21m 33\n",
    "\tbest train_loss: 2.33\n",
    "\tbest eval_loss: 2.318\n",
    "\n",
    "6. imagewoof-noisy5-resnet50\n",
    "\truntime: 3h 4m 26s\n",
    "\tbest train_loss: 2.239\n",
    "\tbest eval_loss: 2.288\n",
    "\n",
    "7. imagewoof-noisy50-resnet18\n",
    "\truntime: 2h 21m 37s\n",
    "\tbest train_loss: 2.364\n",
    "\tbest eval_loss: 2.333\n",
    "\n",
    "8. imagewoof-noisy50-resnet34\n",
    "\truntime: 3h 18m 59s\n",
    "\tbest train_loss: 2.344\n",
    "\tbest eval_loss: 2.326\n",
    "\n",
    "9. imagewoof-noisy50-resnet50\n",
    "\truntime: 2h 29m 16s\n",
    "\tbest train_loss: 2.301\n",
    "\tbest eval_loss: 2.298\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "                  module name  input shape output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                       conv1    3 224 224   64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       1.60%    3851008.0\n",
      "1                         bn1   64 112 112   64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.42%    6423040.0\n",
      "2                        relu   64 112 112   64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.23%    6422528.0\n",
      "3                     maxpool   64 112 112   64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       0.91%    4014080.0\n",
      "4              layer1.0.conv1   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       4.28%    1753088.0\n",
      "5                layer1.0.bn1   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.29%    1606144.0\n",
      "6               layer1.0.relu   64  56  56   64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.22%    1605632.0\n",
      "7              layer1.0.conv2   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       6.71%    1753088.0\n",
      "8                layer1.0.bn2   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.33%    1606144.0\n",
      "9              layer1.1.conv1   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.99%    1753088.0\n",
      "10               layer1.1.bn1   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.49%    1606144.0\n",
      "11              layer1.1.relu   64  56  56   64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.18%    1605632.0\n",
      "12             layer1.1.conv2   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.00%    1753088.0\n",
      "13               layer1.1.bn2   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.26%    1606144.0\n",
      "14             layer2.0.conv1   64  56  56  128  28  28     73728.0       0.38    115,505,152.0     57,802,752.0   1097728.0     401408.0       2.27%    1499136.0\n",
      "15               layer2.0.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.38%     803840.0\n",
      "16              layer2.0.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.22%     802816.0\n",
      "17     layer2.0.conv2.get_q.0  128  28  28   64  28  28      8192.0       0.19     12,794,880.0      6,422,528.0    434176.0     200704.0       1.15%     634880.0\n",
      "18     layer2.0.conv2.get_q.1   64  28  28   64  28  28       128.0       0.19        200,704.0        100,352.0    201216.0     200704.0       0.34%     401920.0\n",
      "19     layer2.0.conv2.get_v.0  128  28  28   32  28  28      4096.0       0.10      6,397,440.0      3,211,264.0    417792.0     100352.0       1.65%     518144.0\n",
      "20     layer2.0.conv2.get_v.1   32  28  28   32  28  28        64.0       0.10        100,352.0         50,176.0    100608.0     100352.0       0.80%     200960.0\n",
      "21       layer2.0.conv2.get_k  128  28  28   16  28  28      2048.0       0.05      3,198,720.0      1,605,632.0    409600.0      50176.0       0.29%     459776.0\n",
      "22               layer2.0.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.34%     803840.0\n",
      "23      layer2.0.downsample.0   64  56  56  128  28  28      8192.0       0.38     12,744,704.0      6,422,528.0    835584.0     401408.0       0.55%    1236992.0\n",
      "24      layer2.0.downsample.1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.30%     803840.0\n",
      "25             layer2.1.conv1  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       2.89%    1392640.0\n",
      "26               layer2.1.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.28%     803840.0\n",
      "27              layer2.1.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.31%     802816.0\n",
      "28     layer2.1.conv2.get_q.0  128  28  28   64  28  28      8192.0       0.19     12,794,880.0      6,422,528.0    434176.0     200704.0       0.29%     634880.0\n",
      "29     layer2.1.conv2.get_q.1   64  28  28   64  28  28       128.0       0.19        200,704.0        100,352.0    201216.0     200704.0       0.25%     401920.0\n",
      "30     layer2.1.conv2.get_v.0  128  28  28   32  28  28      4096.0       0.10      6,397,440.0      3,211,264.0    417792.0     100352.0       0.25%     518144.0\n",
      "31     layer2.1.conv2.get_v.1   32  28  28   32  28  28        64.0       0.10        100,352.0         50,176.0    100608.0     100352.0       0.24%     200960.0\n",
      "32       layer2.1.conv2.get_k  128  28  28   16  28  28      2048.0       0.05      3,198,720.0      1,605,632.0    409600.0      50176.0       0.25%     459776.0\n",
      "33               layer2.1.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.32%     803840.0\n",
      "34             layer2.2.conv1  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       2.20%    1392640.0\n",
      "35               layer2.2.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.27%     803840.0\n",
      "36              layer2.2.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.19%     802816.0\n",
      "37     layer2.2.conv2.get_q.0  128  28  28   64  28  28      8192.0       0.19     12,794,880.0      6,422,528.0    434176.0     200704.0       0.28%     634880.0\n",
      "38     layer2.2.conv2.get_q.1   64  28  28   64  28  28       128.0       0.19        200,704.0        100,352.0    201216.0     200704.0       0.24%     401920.0\n",
      "39     layer2.2.conv2.get_v.0  128  28  28   32  28  28      4096.0       0.10      6,397,440.0      3,211,264.0    417792.0     100352.0       0.31%     518144.0\n",
      "40     layer2.2.conv2.get_v.1   32  28  28   32  28  28        64.0       0.10        100,352.0         50,176.0    100608.0     100352.0       0.24%     200960.0\n",
      "41       layer2.2.conv2.get_k  128  28  28   16  28  28      2048.0       0.05      3,198,720.0      1,605,632.0    409600.0      50176.0       0.26%     459776.0\n",
      "42               layer2.2.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.30%     803840.0\n",
      "43             layer3.0.conv1  128  28  28  256  14  14    294912.0       0.19    115,555,328.0     57,802,752.0   1581056.0     200704.0       5.13%    1781760.0\n",
      "44               layer3.0.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.31%     403456.0\n",
      "45              layer3.0.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.19%     401408.0\n",
      "46             layer3.0.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       2.40%    2760704.0\n",
      "47               layer3.0.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.81%     403456.0\n",
      "48      layer3.0.downsample.0  128  28  28  256  14  14     32768.0       0.19     12,794,880.0      6,422,528.0    532480.0     200704.0       0.85%     733184.0\n",
      "49      layer3.0.downsample.1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.27%     403456.0\n",
      "50             layer3.1.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       6.85%    2760704.0\n",
      "51               layer3.1.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.42%     403456.0\n",
      "52              layer3.1.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.18%     401408.0\n",
      "53             layer3.1.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       5.60%    2760704.0\n",
      "54               layer3.1.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.28%     403456.0\n",
      "55             layer3.2.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.54%    2760704.0\n",
      "56               layer3.2.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.25%     403456.0\n",
      "57              layer3.2.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.18%     401408.0\n",
      "58             layer3.2.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       2.06%    2760704.0\n",
      "59               layer3.2.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.24%     403456.0\n",
      "60             layer3.3.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       4.99%    2760704.0\n",
      "61               layer3.3.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.31%     403456.0\n",
      "62              layer3.3.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.17%     401408.0\n",
      "63             layer3.3.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.66%    2760704.0\n",
      "64               layer3.3.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.25%     403456.0\n",
      "65             layer3.4.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.43%    2760704.0\n",
      "66               layer3.4.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.33%     403456.0\n",
      "67              layer3.4.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.36%     401408.0\n",
      "68             layer3.4.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       4.50%    2760704.0\n",
      "69               layer3.4.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.85%     403456.0\n",
      "70             layer4.0.conv1  256  14  14  512   7   7   1179648.0       0.10    115,580,416.0     57,802,752.0   4919296.0     100352.0       1.04%    5019648.0\n",
      "71               layer4.0.bn1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.27%     204800.0\n",
      "72              layer4.0.relu  512   7   7  512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.19%     200704.0\n",
      "73             layer4.0.conv2  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       6.71%    9637888.0\n",
      "74               layer4.0.bn2  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.88%     204800.0\n",
      "75      layer4.0.downsample.0  256  14  14  512   7   7    131072.0       0.10     12,819,968.0      6,422,528.0    724992.0     100352.0       0.44%     825344.0\n",
      "76      layer4.0.downsample.1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.26%     204800.0\n",
      "77             layer4.1.conv1  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       6.64%    9637888.0\n",
      "78               layer4.1.bn1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.34%     204800.0\n",
      "79              layer4.1.relu  512   7   7  512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.17%     200704.0\n",
      "80             layer4.1.conv2  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       2.96%    9637888.0\n",
      "81               layer4.1.bn2  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.26%     204800.0\n",
      "82                    avgpool  512   7   7  512   1   1         0.0       0.00              0.0              0.0         0.0          0.0       0.50%          0.0\n",
      "83                         fc          512           10      5130.0       0.00         10,230.0          5,120.0     22568.0         40.0       0.35%      22608.0\n",
      "total                                                    14620298.0      31.15  4,863,933,686.0  2,434,343,936.0     22568.0         40.0     100.00%  127575632.0\n",
      "==================================================================================================================================================================\n",
      "Total params: 14,620,298\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 31.15MB\n",
      "Total MAdd: 4.86GMAdd\n",
      "Total Flops: 2.43GFlops\n",
      "Total MemR+W: 121.67MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuanli/miniforge3/envs/torch/lib/python3.8/site-packages/torchstat/reporter.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(total_df)\n"
     ]
    }
   ],
   "source": [
    "from modeling.ResNet import resnet18, resnet34\n",
    "import torch \n",
    "from torchstat import stat\n",
    "from torchvision import models\n",
    "# model = models.resnet34()\n",
    "model = resnet34()\n",
    "# model = resnet34()\n",
    "# model.load_state_dict(torch.load('models/LambdaR18_0_lambda2_r_None.pt',map_location=torch.device('cpu')))\n",
    "# # model = model.layer2[-1]\n",
    "# model.eval()\n",
    "stat(model, (3,224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11689512\n",
      "21797672\n"
     ]
    }
   ],
   "source": [
    "# L_R_18 = resnet18()\n",
    "# L_R_34 = resnet34()\n",
    "\n",
    "# print(sum([i.numel() for i in L_R_18.parameters()]))\n",
    "# print(sum([i.numel() for i in L_R_34.parameters()]))\n",
    "\n",
    "res18 = timm.create_model('resnet18',)\n",
    "res34 = timm.create_model('resnet34',)\n",
    "\n",
    "print(sum([i.numel() for i in res18.parameters()]))\n",
    "print(sum([i.numel() for i in res34.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--use-cuda] [--image-path IMAGE_PATH]\n",
      "                             [--aug_smooth] [--eigen_smooth]\n",
      "                             [--method {gradcam,gradcam++,scorecam,xgradcam,ablationcam,eigencam,eigengradcam}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/xuanli/Library/Jupyter/runtime/kernel-25e1cd1e-8866-4777-84ce-6f3c05cf6e2e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuanli/miniforge3/envs/torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "                             ScoreCAM, \\\n",
    "                             GradCAMPlusPlus, \\\n",
    "                             AblationCAM, \\\n",
    "                             XGradCAM, \\\n",
    "                             EigenCAM, \\\n",
    "                             EigenGradCAM\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "                                         deprocess_image, \\\n",
    "                                         preprocess_image\n",
    "\n",
    "\n",
    "# 如果出现 OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--use-cuda', action='store_true', default=False,\n",
    "                        help='Use NVIDIA GPU acceleration')\n",
    "    parser.add_argument('--image-path', type=str, default='dog.JPEG',\n",
    "                        help='Input image path')\n",
    "    parser.add_argument('--aug_smooth', action='store_true',\n",
    "                        help='Apply test time augmentation to smooth the CAM')\n",
    "    parser.add_argument('--eigen_smooth', action='store_true',\n",
    "                        help='Reduce noise by taking the first principle componenet'\n",
    "                        'of cam_weights*activations')\n",
    "    parser.add_argument('--method', type=str, default='gradcam',\n",
    "                        choices=['gradcam', 'gradcam++', 'scorecam', 'xgradcam',\n",
    "                                 'ablationcam', 'eigencam', 'eigengradcam'],\n",
    "                        help='Can be gradcam/gradcam++/scorecam/xgradcam'\n",
    "                             '/ablationcam/eigencam/eigengradcam')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "    if args.use_cuda:\n",
    "        print('Using GPU for acceleration')\n",
    "    else:\n",
    "        print('Using CPU for computation')\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python cam.py -image-path <path_to_image>\n",
    "    Example usage of loading an image, and computing:\n",
    "        1. CAM\n",
    "        2. Guided Back Propagation\n",
    "        3. Combining both\n",
    "    \"\"\"\n",
    "\n",
    "    args = get_args()\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM}\n",
    "\n",
    "    model = timm.create_model('resnet18',pretrained=True)\n",
    "\n",
    "    # Choose the target layer you want to compute the visualization for.\n",
    "    # Usually this will be the last convolutional layer in the model.\n",
    "    # Some common choices can be:\n",
    "    # Resnet18 and 50: model.layer4[-1]\n",
    "    # VGG, densenet161: model.features[-1]\n",
    "    # mnasnet1_0: model.layers[-1]\n",
    "    # You can print the model to help chose the layer\n",
    "    target_layer = model.layer4[-1]\n",
    "\n",
    "    cam = methods[args.method](model=model,\n",
    "                               target_layer=target_layer,\n",
    "                               use_cuda=args.use_cuda)\n",
    "\n",
    "    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img, mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # If None, returns the map for the highest scoring category.\n",
    "    # Otherwise, targets the requested category.\n",
    "    target_category = None\n",
    "\n",
    "    # AblationCAM and ScoreCAM have batched implementations.\n",
    "    # You can override the internal batch size for faster computation.\n",
    "    cam.batch_size = 32\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        target_category=target_category,\n",
    "                        aug_smooth=args.aug_smooth,\n",
    "                        eigen_smooth=args.eigen_smooth)\n",
    "\n",
    "    # Here grayscale_cam has only one image in the batch\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "\n",
    "    gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)\n",
    "    gb = gb_model(input_tensor, target_category=target_category)\n",
    "\n",
    "    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n",
    "    cam_gb = deprocess_image(cam_mask * gb)\n",
    "    gb = deprocess_image(gb)\n",
    "\n",
    "    cv2.imwrite(f'{args.method}_cam.jpg', cam_image)\n",
    "    cv2.imwrite(f'{args.method}_gb.jpg', gb)\n",
    "    cv2.imwrite(f'{args.method}_cam_gb.jpg', cam_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "                 module name  input shape output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                      conv1    3 224 224   64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       0.82%    3851008.0\n",
      "1                        bn1   64 112 112   64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.12%    6423040.0\n",
      "2                       relu   64 112 112   64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.06%    6422528.0\n",
      "3                    maxpool   64 112 112   64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       0.33%    4014080.0\n",
      "4             layer1.0.conv1   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       2.90%    1753088.0\n",
      "5               layer1.0.bn1   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.09%    1606144.0\n",
      "6              layer1.0.relu   64  56  56   64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
      "7             layer1.0.conv2   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.66%    1753088.0\n",
      "8               layer1.0.bn2   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.08%    1606144.0\n",
      "9             layer1.1.conv1   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       2.27%    1753088.0\n",
      "10              layer1.1.bn1   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.15%    1606144.0\n",
      "11             layer1.1.relu   64  56  56   64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
      "12            layer1.1.conv2   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.17%    1753088.0\n",
      "13              layer1.1.bn2   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.11%    1606144.0\n",
      "14            layer1.2.conv1   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.63%    1753088.0\n",
      "15              layer1.2.bn1   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.08%    1606144.0\n",
      "16             layer1.2.relu   64  56  56   64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
      "17            layer1.2.conv2   64  56  56   64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.49%    1753088.0\n",
      "18              layer1.2.bn2   64  56  56   64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.08%    1606144.0\n",
      "19            layer2.0.conv1   64  56  56  128  28  28     73728.0       0.38    115,505,152.0     57,802,752.0   1097728.0     401408.0       0.39%    1499136.0\n",
      "20              layer2.0.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.07%     803840.0\n",
      "21             layer2.0.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.07%     802816.0\n",
      "22            layer2.0.conv2  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.89%    1392640.0\n",
      "23              layer2.0.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0\n",
      "24     layer2.0.downsample.0   64  56  56  128  28  28      8192.0       0.38     12,744,704.0      6,422,528.0    835584.0     401408.0       1.75%    1236992.0\n",
      "25     layer2.0.downsample.1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0\n",
      "26            layer2.1.conv1  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.58%    1392640.0\n",
      "27              layer2.1.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.26%     803840.0\n",
      "28             layer2.1.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.14%     802816.0\n",
      "29            layer2.1.conv2  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.59%    1392640.0\n",
      "30              layer2.1.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.27%     803840.0\n",
      "31            layer2.2.conv1  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.23%    1392640.0\n",
      "32              layer2.2.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0\n",
      "33             layer2.2.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.05%     802816.0\n",
      "34            layer2.2.conv2  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.64%    1392640.0\n",
      "35              layer2.2.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0\n",
      "36            layer2.3.conv1  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.56%    1392640.0\n",
      "37              layer2.3.bn1  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.26%     803840.0\n",
      "38             layer2.3.relu  128  28  28  128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.06%     802816.0\n",
      "39            layer2.3.conv2  128  28  28  128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.64%    1392640.0\n",
      "40              layer2.3.bn2  128  28  28  128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.22%     803840.0\n",
      "41            layer3.0.conv1  128  28  28  256  14  14    294912.0       0.19    115,555,328.0     57,802,752.0   1581056.0     200704.0       2.35%    1781760.0\n",
      "42              layer3.0.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.12%     403456.0\n",
      "43             layer3.0.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.05%     401408.0\n",
      "44            layer3.0.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.75%    2760704.0\n",
      "45              layer3.0.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.25%     403456.0\n",
      "46     layer3.0.downsample.0  128  28  28  256  14  14     32768.0       0.19     12,794,880.0      6,422,528.0    532480.0     200704.0       0.25%     733184.0\n",
      "47     layer3.0.downsample.1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.09%     403456.0\n",
      "48            layer3.1.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.72%    2760704.0\n",
      "49              layer3.1.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.20%     403456.0\n",
      "50             layer3.1.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.05%     401408.0\n",
      "51            layer3.1.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       3.63%    2760704.0\n",
      "52              layer3.1.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.08%     403456.0\n",
      "53            layer3.2.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.70%    2760704.0\n",
      "54              layer3.2.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0\n",
      "55             layer3.2.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.07%     401408.0\n",
      "56            layer3.2.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       2.64%    2760704.0\n",
      "57              layer3.2.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.11%     403456.0\n",
      "58            layer3.3.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.93%    2760704.0\n",
      "59              layer3.3.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.12%     403456.0\n",
      "60             layer3.3.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.15%     401408.0\n",
      "61            layer3.3.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       3.23%    2760704.0\n",
      "62              layer3.3.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.27%     403456.0\n",
      "63            layer3.4.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.59%    2760704.0\n",
      "64              layer3.4.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.26%     403456.0\n",
      "65             layer3.4.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.06%     401408.0\n",
      "66            layer3.4.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.73%    2760704.0\n",
      "67              layer3.4.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0\n",
      "68            layer3.5.conv1  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.69%    2760704.0\n",
      "69              layer3.5.bn1  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.08%     403456.0\n",
      "70             layer3.5.relu  256  14  14  256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.05%     401408.0\n",
      "71            layer3.5.conv2  256  14  14  256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.91%    2760704.0\n",
      "72              layer3.5.bn2  256  14  14  256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.09%     403456.0\n",
      "73            layer4.0.conv1  256  14  14  512   7   7   1179648.0       0.10    115,580,416.0     57,802,752.0   4919296.0     100352.0       0.52%    5019648.0\n",
      "74              layer4.0.bn1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
      "75             layer4.0.relu  512   7   7  512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.05%     200704.0\n",
      "76            layer4.0.conv2  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       3.01%    9637888.0\n",
      "77              layer4.0.bn2  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.12%     204800.0\n",
      "78     layer4.0.downsample.0  256  14  14  512   7   7    131072.0       0.10     12,819,968.0      6,422,528.0    724992.0     100352.0       0.45%     825344.0\n",
      "79     layer4.0.downsample.1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.13%     204800.0\n",
      "80            layer4.1.conv1  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.16%    9637888.0\n",
      "81              layer4.1.bn1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0\n",
      "82             layer4.1.relu  512   7   7  512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.05%     200704.0\n",
      "83            layer4.1.conv2  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.81%    9637888.0\n",
      "84              layer4.1.bn2  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0\n",
      "85            layer4.2.conv1  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0      27.63%    9637888.0\n",
      "86              layer4.2.bn1  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.14%     204800.0\n",
      "87             layer4.2.relu  512   7   7  512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.05%     200704.0\n",
      "88            layer4.2.conv2  512   7   7  512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       4.44%    9637888.0\n",
      "89              layer4.2.bn2  512   7   7  512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.09%     204800.0\n",
      "90                   avgpool  512   7   7  512   1   1         0.0       0.00              0.0              0.0         0.0          0.0      17.15%          0.0\n",
      "91                        fc          512         1000    513000.0       0.00      1,023,000.0        512,000.0   2054048.0       4000.0       0.20%    2058048.0\n",
      "total                                                   21797672.0      37.62  7,342,524,440.0  3,674,223,104.0   2054048.0       4000.0     100.00%  167277632.0\n",
      "=================================================================================================================================================================\n",
      "Total params: 21,797,672\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 37.62MB\n",
      "Total MAdd: 7.34GMAdd\n",
      "Total Flops: 3.67GFlops\n",
      "Total MemR+W: 159.53MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuanli/miniforge3/envs/torch/lib/python3.8/site-packages/torchstat/reporter.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(total_df)\n"
     ]
    }
   ],
   "source": [
    "from modeling.ResNet import resnet18, resnet34\n",
    "import torch \n",
    "from torchstat import stat\n",
    "from torchvision import models\n",
    "model = models.resnet34()\n",
    "# model = resnet18()\n",
    "# model.load_state_dict(torch.load('models/LambdaR18_0_lambda2_r_None.pt',map_location=torch.device('cpu')))\n",
    "# # model = model.layer2[-1]\n",
    "# model.eval()\n",
    "stat(model, (3,224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet50ts_a1h_256-b87370f7.pth\" to /Users/xuanli/.cache/torch/hub/checkpoints/lambda_resnet50ts_a1h_256-b87370f7.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('lambda_resnet50ts', pretrained=True)\n",
    "model.final_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/f44w08g92cddctb47s9_02jw0000gn/T/ipykernel_7316/572742062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"backspace\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/pyautogui/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mfailSafeCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mreturnVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappedFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0m_handlePause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_pause\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturnVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/pyautogui/__init__.py\u001b[0m in \u001b[0;36mtypewrite\u001b[0;34m(message, interval, logScreenshot, _pause)\u001b[0m\n\u001b[1;32m   1679\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0mpress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m         \u001b[0mfailSafeCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyautogui as auto \n",
    "while(1): \n",
    "    auto.typewrite('A',interval=1)  \n",
    "    auto.typewrite([\"backspace\"],interval=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bottleneck(\n",
       "  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU(inplace=True)\n",
       "  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm \n",
    "model = timm.create_model('resnet50')\n",
    "# model.stages[-1][-1].conv3_1x1.bn\n",
    "# model.layer4[-1]\n",
    "model.layer2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lambda18_l2_r_none'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import get_config\n",
    "\n",
    "get_config('lambda18_l2_r_none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): LambdaLayer(\n",
       "        (get_q): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_v): Sequential(\n",
       "          (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_k): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): LambdaLayer(\n",
       "        (get_q): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_v): Sequential(\n",
       "          (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_k): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): LambdaLayer(\n",
       "        (get_q): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_v): Sequential(\n",
       "          (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (get_k): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock_Rs(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet18, resnet34\n",
    "model = resnet34()\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "099d15d9623218e05815a9aa435e69d26d96cf35f579f62cb4dcfa91bbc85db9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
