{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-10T06:11:43.884281Z","iopub.status.busy":"2022-05-10T06:11:43.883572Z","iopub.status.idle":"2022-05-10T06:11:59.027213Z","shell.execute_reply":"2022-05-10T06:11:59.026087Z","shell.execute_reply.started":"2022-05-10T06:11:43.884175Z"},"trusted":true},"outputs":[],"source":["!pip install einops timm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SoftCrossEntropyLoss():\n","    def __init__(self, weights):\n","        super().__init__()\n","        self.weights = weights\n","\n","def forward(self, y_hat, y):\n","    p = F.log_softmax(y_hat, 1)\n","    w_labels = self.weights*y\n","    loss = -(w_labels*p).sum() / (w_labels).sum()\n","    return loss\n","\n","x = torch.randn(1,3)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:55:05.387109Z","iopub.status.busy":"2022-05-10T11:55:05.386645Z","iopub.status.idle":"2022-05-10T14:38:55.813992Z","shell.execute_reply":"2022-05-10T14:38:55.812896Z","shell.execute_reply.started":"2022-05-10T11:55:05.387027Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from einops import rearrange, repeat\n","# from lambda_networks import LambdaLayer\n","from torch import einsum\n","# make required import\n","import torch \n","import timm \n","import wandb \n","import os\n","import torchvision\n","import numpy as np \n","import pandas as pd\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from accelerate import Accelerator\n","from PIL import Image \n","from matplotlib import pyplot as plt\n","from pathlib import Path \n","from tqdm import tqdm\n","from sklearn.preprocessing import LabelEncoder\n","from dataclasses import dataclass\n","from timm.models.registry import register_model\n","from timm.models.helpers import build_model_with_cfg\n","from timm.models.resnet import Bottleneck, _create_resnet, default_cfgs, _cfg, make_blocks, create_classifier\n","import torch\n","import torch.nn as nn\n","from einops import rearrange, repeat\n","# from lambda_networks import LambdaLayer\n","from torch import einsum\n","# from lambda_networks import LambdaLayer\n","\n","# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n","def get_relative_position_matrix(size,):\n","    x = torch.repeat_interleave(torch.arange(size),size,dim=0)\n","    y = torch.arange(size).repeat(size)\n","    distance_mat = torch.stack((x,y))\n","    distance_mat = distance_mat[:,None, :] - distance_mat[:, :,None]\n","    distance_mat = torch.clamp(distance_mat, -size, size)\n","    distance_mat += size-1\n","    return distance_mat\n","\n","class LambdaLayer_own(nn.Module):\n","    def __init__(self, dim, dim_k=16,n=64, dim_out=None, heads=4, r=None):\n","        super().__init__()\n","        self.dim_out = dim_out\n","        self.r = r\n","        self.n = n\n","        self.k = dim_k\n","        assert(dim_out % heads) == 0\n","        self.heads = heads\n","        self.dim_v = dim_out // heads\n","\n","# (Page 5) BN after Q and V are helpful\n","        self.get_q = nn.Sequential(\n","            nn.Conv2d(in_channels=dim, out_channels=dim_k*heads, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(dim_k*heads),\n","        )\n","        self.get_v = nn.Sequential(\n","            nn.Conv2d(in_channels=dim, out_channels=self.dim_v, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(self.dim_v),\n","        )\n","        self.get_k = nn.Conv2d(in_channels=dim, out_channels=dim_k, kernel_size=1, bias=False)\n","\n","        if r is not None:\n","            assert (r % 2) == 1, 'Receptive kernel size should be odd'\n","            self.pos_conv = nn.Conv3d(1, self.k, (r, r, 1), padding=(r // 2, r // 2, 0))\n","        else:   \n","            # rel_lengths = 2 * n - 1 # n = im_h = im_w the feature map size\n","            # self.rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, dim_k)) # n m k \n","            self.rel_pos = get_relative_position_matrix(n)\n","            rel_lengths = 2 * self.n - 1 # n = im_h = im_w the feature map size\n","            self.rel_pos_emb = nn.Parameter(torch.randn(rel_lengths, rel_lengths, self.k)) # 2*n-1 2*n-1 k \n","\n","    def forward(self,x):\n","        (b, c, im_h, im_w) = x.shape\n","        Q = self.get_q(x)\n","        K = self.get_k(x)\n","        V = self.get_v(x)\n","\n","        # m = n = im_h * im_w\n","        Q = rearrange(Q, 'b (h k) im_h im_w -> b h (im_h im_w) k', h=self.heads) # b h n(m) k \n","        K = rearrange(K, 'b k im_h im_w -> b (im_h im_w) k') # b m k \n","        V = rearrange(V, 'b v im_h im_w -> b (im_h im_w) v') # b m v\n","\n","        σ_K = K.softmax(dim=-1)\n","        λc = einsum('b m k, b m v -> b k v', σ_K, V)\n","        content_output = einsum('b h n k, b k v -> b n h v', Q, λc)\n","\n","        if self.r is not None:\n","            V = rearrange(V, 'b (im_h im_w) v -> b v im_h im_w', im_h = im_h, im_w = im_w).reshape(b, 1, self.dim_v, im_h, im_w,)\n","            embeddings = self.pos_conv(V)\n","            position_output = einsum('b h n k, b k v n -> b n h v', Q, embeddings.flatten(3))\n","        else:\n","            # embeddings = get_embedding(self.k, im_h) # n m k\n","            embeddings = self.rel_pos_emb[self.rel_pos[0], self.rel_pos[1]]\n","#             print(embeddings.shape)\n","#             print(V.shape)\n","            λp = einsum('n m k, b m v -> b n k v', embeddings, V)\n","            position_output = einsum('b h n k, b n k v -> b n h v',Q, λp)\n","\n","        output = rearrange(content_output + position_output, 'b (im_h im_w) h v -> b (h v) im_h im_w', im_h=im_h, im_w=im_w)\n","\n","        return output\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None,feature_size=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        # self.conv2 = conv3x3(planes, planes)\n","#         self.conv2 = LambdaLayer(\n","#                             dim = planes,\n","#                             dim_out = planes,\n","#                             n = feature_size,\n","# #                             r = 23,         # the receptive field for relative positional encoding (23 x 23)\n","#                             dim_k = 16,\n","#                             heads = 4,\n","#                             dim_u = 4\n","#                         )\n","        self.conv2 = LambdaLayer_own(dim=planes, dim_out=planes, n=feature_size)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class BasicBlock_Rs(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None,feature_size=None):\n","        super(BasicBlock_Rs, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        # self.conv2 = LambdaLayer(dim=planes, dim_out=planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","    \n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block_lambda, block_rs, layers, num_classes=10, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None,):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        # self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n","        #                        bias=False)\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block_rs, 64, layers[0], feature_size=56)\n","        self.layer2 = self._make_layer(block_lambda, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0],feature_size=28) # 56/2=\n","        self.layer3 = self._make_layer(block_rs, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1],feature_size=14) # 14\n","        self.layer4 = self._make_layer(block_rs, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2],feature_size=7) # 7\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block_rs.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False, feature_size=None):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer, feature_size=feature_size))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer,feature_size=feature_size))\n","\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","\n","def _resnet(arch, block,BasicBlock_Rs, layers, pretrained, progress, **kwargs):\n","    model = ResNet(block, BasicBlock_Rs, layers, **kwargs)\n","    # if pretrained:\n","    #     state_dict = load_state_dict_from_url(model_urls[arch],\n","    #                                           progress=progress)\n","    #     model.load_state_dict(state_dict)\n","    return model\n","\n","\n","def resnet18(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-18 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet18', BasicBlock,BasicBlock_Rs, [2, 2, 2, 2], pretrained, progress,\n","                   **kwargs)\n","\n","# [2, 2, 2, 2] RL-26\n","# [2, 3, 5, 2] RL-38\n","# [3, 4, 6, 3] RL-50\n","\n","def resnet34(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-34 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet34', BasicBlock,BasicBlock_Rs, [2, 3, 5, 2], pretrained, progress,\n","                   **kwargs)\n","\n","\n","\n","def resnet50(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-50 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet50', BasicBlock, BasicBlock_Rs,[3, 4, 6, 3], pretrained, progress,\n","                   **kwargs)\n","\n","\n","class ImageNette(Dataset):\n","    \n","    def __init__(self, csv_file, root_dir, noisy_level=0, transform=None, train=True):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.image_path_list = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.noisy_level = noisy_level\n","        self.train = train\n","        self.train_map = {\n","             'n02115641': 0,\n","             'n02086240': 1,\n","             'n02088364': 2,\n","             'n02087394': 3,\n","             'n02105641': 4,\n","             'n02111889': 5,\n","             'n02099601': 6,\n","             'n02096294': 7,\n","             'n02093754': 8,\n","             'n02089973': 9\n","        }\n","#         { ImageNette\n","#             'n02979186': 0,\n","#             'n03417042': 1,\n","#             'n01440764': 2,\n","#             'n02102040': 3,\n","#             'n03028079': 4,\n","#             'n03888257': 5,\n","#             'n03394916': 6,\n","#             'n03000684': 7,\n","#             'n03445777': 8,\n","#             'n03425413': 9\n","#         }\n","        self.val_map ={\n","             'n02115641': 0,\n","             'n02086240': 1,\n","             'n02088364': 2,\n","             'n02087394': 3,\n","             'n02105641': 4,\n","             'n02111889': 5,\n","             'n02099601': 6,\n","             'n02096294': 7,\n","             'n02093754': 8,\n","             'n02089973': 9\n","        } \n","    \n","#     { ImageNette\n","#             'n02979186': 0,\n","#             'n03417042': 1,\n","#             'n01440764': 2,\n","#             'n02102040': 3,\n","#             'n03028079': 4,\n","#             'n03888257': 5,\n","#             'n03394916': 6,\n","#             'n03000684': 7,\n","#             'n03445777': 8,\n","#             'n03425413': 9\n","#         }\n","\n","\n","    def __len__(self):\n","        return len(self.image_path_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_path = os.path.join(self.root_dir,\n","                                self.image_path_list['path'][idx])\n","        image = Image.open(img_path).convert('RGB')\n","        labels_head = 'noisy_labels_'+str(self.noisy_level)\n","        if self.train:\n","            label = self.train_map[self.image_path_list[labels_head][idx]]\n","        else:\n","            label = self.val_map[self.image_path_list[labels_head][idx]]\n","        label = torch.from_numpy(np.array([label]))\n","\n","        if self.transform:\n","#             try:\n","            image = self.transform(image)\n","#             except:\n","#                 print(img_path)\n","        sample =  [image, label]\n","        return sample\n","\n","\n","# Config for training \n","IMG_SIZE = 224\n","\n","Config = dict(\n","    DATA_DIR=\"../input/imagenette/imagenette\",\n","    TRAIN_DATA_DIR=\"../input/imagenette/imagenette/train\",\n","    TEST_DATA_DIR=\"../input/imagenette/imagenette/val\",\n","    DEVICE=\"cuda\",\n","    PRETRAINED=False,\n","    LR=1e-5,\n","    EPOCHS=50,\n","    IMG_SIZE=IMG_SIZE,\n","    BS=64,\n","    TRAIN_AUG=transforms.Compose(\n","        [\n","            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.RandomRotation(15),\n","            transforms.ToTensor(),\n","            transforms.RandomErasing(0.2),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","        ]\n","    ),\n","    TEST_AUG=transforms.Compose(\n","        [\n","            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","        ]\n","    ),\n",")\n","\n","# image_path_label = pd.read_csv(csv_file)\n","# train_path_label = image_path_label.loc[image_path_label['is_valid'] == 0]\n","# val_path_label = image_path_label.loc[image_path_label['is_valid'] == 1]\n","\n","def train_fn(model, train_data_loader, optimizer, epoch, accelerator,scheduler):\n","    model.train()\n","    fin_loss = 0.0\n","    tk = tqdm(train_data_loader, desc=\"Epoch\" + \" [TRAIN] \" + str(epoch + 1))\n","\n","    for t, data in enumerate(tk):\n","        optimizer.zero_grad()\n","        out = model(data[0])\n","#         print(data[1].shape)\n","        loss = nn.CrossEntropyLoss()(\n","            out, data[1].flatten()\n","            )\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        scheduler.step(epoch + t / len(train_data_loader))\n","        fin_loss += loss.item()\n","        tk.set_postfix(\n","            {\n","                \"loss\": \"%.6f\" % float(fin_loss / (t + 1)),\n","                \"LR\": optimizer.param_groups[0][\"lr\"],\n","            }\n","        )\n","    return fin_loss/len(train_data_loader), optimizer.param_groups[0][\"lr\"]\n","\n","def eval_fn(model, eval_data_loader, epoch):\n","    model.eval()\n","    fin_loss = 0.0\n","    tk = tqdm(eval_data_loader, desc=\"Epoch\" + \" [VALID] \" + str(epoch + 1))\n","\n","    with torch.no_grad():\n","        for t, data in enumerate(tk):\n","            out = model(data[0])\n","            loss = nn.CrossEntropyLoss()(\n","                out, data[1].flatten()\n","                )\n","            fin_loss += loss.item()\n","            tk.set_postfix({\"loss\": \"%.6f\" % float(fin_loss / (t + 1))})\n","        return fin_loss/len(eval_data_loader)\n","\n","def train():\n","    accelerator = Accelerator()\n","\n","    # wandb inita\n","    wandb.init(config=Config, project='imagewoof', save_code=True, \n","           job_type='train', tags=['resnetrs', 'imagewoof'], \n","           name='LambdaR18_25_lambda2_r_None')    \n","    \n","    # train and eval datasets \n","#     train_dataset = torchvision.datasets.ImageFolder(\n","#         Config['TRAIN_DATA_DIR'], \n","#         transform=Config['TRAIN_AUG']\n","#         )\n","#     eval_dataset = torchvision.datasets.ImageFolder(\n","#         Config['TEST_DATA_DIR'], \n","#         transform=Config['TEST_AUG']\n","#         )\n","\n","\n","    train_dataset = ImageNette(\n","#         csv_file='../input/imagenette/imagenette/train_noisy_imagenette.csv', \n","        csv_file = '../input/imagewoof/imagewoof2/train_noisy_imagewoof.csv',\n","#         root_dir='../input/imagenette/imagenette', \n","        root_dir = '../input/imagewoof/imagewoof2',\n","        noisy_level=25, # 0 1 5 25 50\n","        transform=Config['TRAIN_AUG'], \n","        train=True \n","    )\n","    eval_dataset = ImageNette(\n","#         csv_file='../input/imagenette/imagenette/val_noisy_imagenette.csv', \n","#         root_dir='../input/imagenette/imagenette', \n","        csv_file = '../input/imagewoof/imagewoof2/val_noisy_imagewoof.csv',\n","        root_dir = '../input/imagewoof/imagewoof2',\n","        noisy_level=0, \n","        transform=Config['TEST_AUG'], \n","        train=False\n","    )\n","\n","    # train and eval dataloaders\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=Config[\"BS\"]\n","    )\n","    eval_dataloader = torch.utils.data.DataLoader(\n","        eval_dataset, batch_size=Config[\"BS\"]\n","    )\n","\n","    # model\n","    model = resnet34().cuda()\n","\n","    # optimizer    \n","    optimizer = torch.optim.Adam(\n","        model.parameters(), lr=Config[\"LR\"]\n","    )\n","    \n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=0)\n","    # prepare for DDP\n","    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)    \n","\n","    for epoch in range(Config[\"EPOCHS\"]):\n","        avg_loss_train, lr = train_fn(\n","            model, train_dataloader, optimizer, epoch, accelerator,scheduler)\n","        avg_loss_eval = eval_fn(\n","            model, eval_dataloader, epoch)\n","        wandb.log({'train_loss': avg_loss_train, 'eval_loss': avg_loss_eval, 'lr': lr})\n","    torch.save(model.state_dict(), 'LambdaR18_25_lambda2_r_None.pt')\n","        \n","train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
